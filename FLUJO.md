# ğŸ”„ Flujo de Trabajo ESDATA_Epsilon

## ğŸ“Š Resumen Ejecutivo

El pipeline ESDATA_Epsilon procesa datos inmobiliarios a travÃ©s de **8 pasos secuenciales**, cada uno con objetivos especÃ­ficos y validaciones automÃ¡ticas. El proceso transforma datos crudos de mÃºltiples fuentes en informaciÃ³n estructurada y confiable para anÃ¡lisis de mercado.

## ğŸ¯ Objetivo General

Procesar y analizar datos inmobiliarios de Guadalajara y Zapopan, MÃ©xico, generando reportes estadÃ­sticos confiables y geoespacialmente precisos para anÃ¡lisis de mercado inmobiliario.

---

## ğŸ“ Pipeline Detallado

### ğŸ”§ Step 1: ConsolidaciÃ³n y AdecuaciÃ³n
**Archivo**: `esdata/pipeline/step1_consolidar_adecuar.py`

#### ğŸ¯ **Objetivo**
Unificar mÃºltiples archivos CSV fuente, normalizar nombres de columnas, estandarizar valores, y extraer precios con conversiÃ³n de divisas.

#### ğŸ“¥ **Entrada**
- MÃºltiples archivos CSV en `Base_de_Datos/{periodo}/`
- Archivo de configuraciÃ³n: `Lista de Variables Orquestacion.csv`

#### ğŸ”„ **Proceso Principal**

1. **Lectura y UnificaciÃ³n**
   ```python
   # Lee todos los CSV del directorio base
   archivos_csv = glob.glob(f"{base_dir}/*.csv")
   df_consolidado = pd.concat([pd.read_csv(f) for f in archivos_csv])
   ```

2. **Mapeo de Columnas**
   ```python
   COLUMN_NAME_MAPPING = {
       'Precio': 'precio',
       'Superficie': 'area_m2', 
       'OperaciÃ³n': 'operacion',
       'Tipo de Propiedad': 'tipo_propiedad',
       # ... mÃ¡s mapeos
   }
   ```

3. **ExtracciÃ³n de Precios**
   ```python
   def _extract_precio(precio_str):
       # Formatos: "rentaUSD 1,650", "ventaMN 10,650,000"
       if 'USD' in precio_str:
           return float(precio_numerico) * 20  # USD â†’ MN
       return float(precio_numerico)
   ```

4. **EstandarizaciÃ³n de Valores**
   - NormalizaciÃ³n usando CSV de configuraciÃ³n
   - Limpieza de caracteres especiales
   - UnificaciÃ³n de formatos

#### ğŸ“¤ **Salida**
- `N1_Tratamiento/Consolidados/{periodo}/1.Consolidado_Adecuado_{periodo}.csv`
- **MÃ©tricas**: Total propiedades, columnas procesadas, precios extraÃ­dos

#### ğŸš¨ **Validaciones**
- âœ… VerificaciÃ³n de columnas requeridas
- âœ… ValidaciÃ³n de formatos de precio
- âœ… Control de duplicaciÃ³n de registros

---

### ğŸ—ºï¸ Step 2: Procesamiento Geoespacial
**Archivo**: `esdata/geo/step2_procesamiento_geoespacial.py`

#### ğŸ¯ **Objetivo**
Asignar colonias y ciudades usando polÃ­gonos GeoJSON, validar coordenadas y mantener coherencia geoespacial.

#### ğŸ“¥ **Entrada**
- `1.Consolidado_Adecuado_{periodo}.csv`
- GeoJSON de colonias: `N1_Tratamiento/Geolocalizacion/GEOJSON/`

#### ğŸ”„ **Proceso Principal**

1. **Carga de GeometrÃ­as**
   ```python
   gdf_colonias = gpd.read_file("colonias.geojson")
   points = gpd.points_from_xy(df.longitud, df.latitud)
   ```

2. **AsignaciÃ³n Espacial**
   ```python
   # Spatial join para asignar colonia
   df_geo = gpd.sjoin(gdf_points, gdf_colonias, how='left', predicate='within')
   ```

3. **ValidaciÃ³n de Coherencia**
   ```python
   # Coherencia: sin colonia â†’ sin ciudad
   sin_colonia_mask = df['colonia'] == 'Desconocido'
   df.loc[sin_colonia_mask, 'Ciudad'] = 'Desconocido'
   ```

#### ğŸ“¤ **Salida**
- `2.Consolidado_ConColonia_{periodo}.csv`
- **MÃ©tricas**: Propiedades con/sin colonia, cobertura por ciudad

#### ğŸš¨ **Validaciones**
- âœ… Coordenadas vÃ¡lidas (lat/lon en rangos)
- âœ… Coherencia geoespacial
- âœ… VerificaciÃ³n de asignaciÃ³n de colonias

---

### ğŸ”€ Step 3: Versiones Especiales
**Archivo**: `esdata/pipeline/step3_versiones_especiales.py`

#### ğŸ¯ **Objetivo**
Generar versiones especÃ­ficas de datos filtrados para diferentes anÃ¡lisis.

#### ğŸ“¥ **Entrada**
- `2.Consolidado_ConColonia_{periodo}.csv`

#### ğŸ”„ **Proceso Principal**
1. Filtrado por criterios especÃ­ficos
2. GeneraciÃ³n de subconjuntos especializados
3. ValidaciÃ³n de consistencia

#### ğŸ“¤ **Salida**
- MÃºltiples archivos CSV especializados
- Versiones para diferentes tipos de anÃ¡lisis

---

### ğŸ“ Step 4: AnÃ¡lisis de Variables de Texto
**Archivo**: `esdata/text/step4_analisis_variables_texto.py`

#### ğŸ¯ **Objetivo**
Procesar y analizar variables textuales, extraer caracterÃ­sticas y patrones.

#### ğŸ“¥ **Entrada**
- Archivos del Step 3

#### ğŸ”„ **Proceso Principal**
1. AnÃ¡lisis de longitud de textos
2. ExtracciÃ³n de caracterÃ­sticas
3. ClasificaciÃ³n de contenido
4. DetecciÃ³n de patrones

#### ğŸ“¤ **Salida**
- Archivos con variables textuales procesadas
- Reportes de anÃ¡lisis de texto

---

### âœ… Step 5: AnÃ¡lisis LÃ³gico y CorroboraciÃ³n
**Archivo**: `esdata/pipeline/step5_analisis_logico_corroboracion.py`

#### ğŸ¯ **Objetivo**
Aplicar filtros de calidad especÃ­ficos por tipo de propiedad y operaciÃ³n, eliminando propiedades que no cumplen criterios lÃ³gicos.

#### ğŸ“¥ **Entrada**
- Archivos procesados del Step 4

#### ğŸ”„ **Proceso Principal**

1. **NormalizaciÃ³n de Operaciones**
   ```python
   def _normalize_operation(op):
       if op in ['venta', 'Venta', 'VENTA']:
           return 'Ven'
       elif op in ['renta', 'Renta', 'RENTA']:
           return 'Ren'
       return op
   ```

2. **AplicaciÃ³n de Filtros por Tipo**
   ```python
   PROPERTY_CONDITIONS = {
       'Cas': {  # Casa
           'venta': {'precio': (500000, 50000000), 'area_m2': (50, 2000)},
           'renta': {'precio': (5000, 100000), 'area_m2': (50, 2000)}
       },
       'Dep': {  # Departamento
           'venta': {'precio': (400000, 30000000), 'area_m2': (25, 800)},
           'renta': {'precio': (4000, 80000), 'area_m2': (25, 800)}
       }
       # ... mÃ¡s tipos
   }
   ```

#### ğŸ“¤ **Salida**
- `Final_Num_{periodo}.csv`
- **MÃ©tricas**: Propiedades filtradas, razones de exclusiÃ³n

#### ğŸš¨ **Validaciones**
- âœ… Rangos lÃ³gicos por tipo de propiedad
- âœ… Consistencia operaciÃ³n-precio
- âœ… Valores atÃ­picos detectados

---

### ğŸ”„ Step 6: RemociÃ³n de Duplicados
**Archivo**: `esdata/pipeline/step6_remover_duplicados.py`

#### ğŸ¯ **Objetivo**
Eliminar propiedades duplicadas manteniendo consistencia entre archivos Final_Num, Final_AME y Final_MKT.

#### ğŸ“¥ **Entrada**
- `Final_Num_{periodo}.csv`
- Archivos relacionados de amenidades y marketing

#### ğŸ”„ **Proceso Principal**

1. **DetecciÃ³n de Duplicados**
   ```python
   # Criterios jerÃ¡rquicos de duplicaciÃ³n
   duplicados = df.groupby(['latitud', 'longitud', 'precio', 'area_m2'])
   ```

2. **PreservaciÃ³n Consistente**
   ```python
   # LEFT JOIN para mantener consistencia
   final_ame = pd.merge(ids_finales, df_ame, on='id', how='left')
   final_mkt = pd.merge(ids_finales, df_mkt, on='id', how='left')
   ```

#### ğŸ“¤ **Salida**
- `Final_Num_{periodo}.csv` (sin duplicados)
- `Final_AME_{periodo}.csv` 
- `Final_MKT_{periodo}.csv`
- **GarantÃ­a**: Misma cantidad de registros en los 3 archivos

#### ğŸš¨ **Validaciones**
- âœ… Consistencia entre archivos finales
- âœ… PreservaciÃ³n de registros Ãºnicos
- âœ… VerificaciÃ³n de conteos

---

### ğŸ“Š Step 7: EstadÃ­sticas de Variables
**Archivo**: `esdata/estadistica/step7_estadisticas_variables.py`

#### ğŸ¯ **Objetivo**
Calcular estadÃ­sticas descriptivas exhaustivas para todas las variables del dataset.

#### ğŸ“¥ **Entrada**
- Archivos Final_{tipo}_{periodo}.csv

#### ğŸ”„ **Proceso Principal**
1. **AnÃ¡lisis por Tipo de Variable**
   - NumÃ©ricas: media, mediana, std, percentiles
   - CategÃ³ricas: frecuencias, modos
   - Textuales: longitudes, patrones

2. **DetecciÃ³n AutomÃ¡tica de Tipos**
   ```python
   def detectar_tipo_variable(serie):
       if serie.dtype in ['int64', 'float64']:
           return 'numerica'
       elif len(serie.unique()) / len(serie) < 0.1:
           return 'categorica'
       return 'textual'
   ```

#### ğŸ“¤ **Salida**
- `N2_Estadisticas/Reportes/{periodo}/estadisticas_variables.csv`
- Reportes especializados por tipo de variable

---

### ğŸ˜ï¸ Step 8: Resumen por Colonias
**Archivo**: `esdata/estadistica/step8_resumen_colonias.py`

#### ğŸ¯ **Objetivo**
Generar reportes especializados por colonia, tablero maestro de cobertura y archivo de puntos finales.

#### ğŸ“¥ **Entrada**
- Archivos finales procesados
- GeoJSON completo de colonias

#### ğŸ”„ **Proceso Principal**

1. **GeneraciÃ³n Final_Puntos**
   ```python
   def generar_final_puntos():
       # Combina TODOS los datos finales SIN filtro de colonia
       df_final = pd.read_csv(final_num_path)
       df_final.to_csv(final_puntos_path)
   ```

2. **Tablero Maestro de Colonias**
   ```python
   def generar_tablero_maestro_colonias():
       # Incluye TODAS las 1,062 colonias del GeoJSON
       cobertura_completa = merge(todas_colonias, datos, how='left')
   ```

3. **EstadÃ­sticas por Colonia**
   - Cantidad de propiedades por colonia
   - Precio promedio/mediano por colonia
   - Tipos de propiedad mÃ¡s comunes
   - MÃ©tricas de mercado

#### ğŸ“¤ **Salida**
- `N5_Resultados/Nivel_1/CSV/Final_Puntos_{periodo}.csv`
- `N2_Estadisticas/Reportes/{periodo}/tablero_maestro_colonias.csv`
- Reportes individuales por colonia

#### ğŸš¨ **Validaciones**
- âœ… Cobertura completa de colonias (1,062)
- âœ… MÃ©tricas de calidad por colonia
- âœ… AnÃ¡lisis de gaps de informaciÃ³n

---

## ğŸ¯ MÃ©tricas de Calidad Global

### ğŸ“ˆ **Indicadores de Rendimiento** (Sep25)

| Paso | Entrada | Salida | Filtrado | Porcentaje |
|------|---------|--------|----------|------------|
| **Step 1** | ~30,000 | 25,851 | 4,149 | 86.2% |
| **Step 2** | 25,851 | 25,815 | 36 | 99.9% |
| **Step 5** | 25,815 | 24,853 | 962 | 96.3% |
| **Step 6** | 24,853 | 24,853 | 0 | 100% |

### ğŸ—ºï¸ **Cobertura Geoespacial**

- **Total Colonias**: 1,062 (GeoJSON)
- **Colonias con Datos**: 423 (39.8%)
- **Zapopan**: 770 colonias â†’ 291 con datos (37.8%)
- **Guadalajara**: 292 colonias â†’ 132 con datos (45.2%)

---

## ğŸš€ EjecuciÃ³n del Pipeline

### âš¡ **EjecuciÃ³n Secuencial**

```bash
#!/bin/bash
# Script de ejecuciÃ³n completa

PERIODO="Sep25"

echo "ğŸš€ Iniciando Pipeline ESDATA_Epsilon para $PERIODO"

echo "ğŸ“Š Step 1: ConsolidaciÃ³n..."
python -m esdata.pipeline.step1_consolidar_adecuar $PERIODO

echo "ğŸ—ºï¸ Step 2: Geoespacial..."
python -m esdata.geo.step2_procesamiento_geoespacial $PERIODO

echo "ğŸ”€ Step 3: Versiones Especiales..."
python -m esdata.pipeline.step3_versiones_especiales $PERIODO

echo "ğŸ“ Step 4: Variables Texto..."
python -m esdata.text.step4_analisis_variables_texto $PERIODO

echo "âœ… Step 5: ValidaciÃ³n LÃ³gica..."
python -m esdata.pipeline.step5_analisis_logico_corroboracion $PERIODO

echo "ğŸ”„ Step 6: Duplicados..."
python -m esdata.pipeline.step6_remover_duplicados $PERIODO

echo "ğŸ“Š Step 7: EstadÃ­sticas..."
python -m esdata.estadistica.step7_estadisticas_variables $PERIODO

echo "ğŸ˜ï¸ Step 8: Colonias..."
python -m esdata.estadistica.step8_resumen_colonias $PERIODO

echo "âœ… Pipeline completado para $PERIODO"
```

### ğŸ” **Monitoreo de Progreso**

Cada paso genera logs detallados con:
- âœ… Emojis para identificaciÃ³n rÃ¡pida
- ğŸ“Š EstadÃ­sticas de entrada/salida
- â±ï¸ Tiempos de ejecuciÃ³n
- ğŸš¨ Alertas de calidad
- ğŸ“ˆ MÃ©tricas de cobertura

### ğŸ› **Manejo de Errores**

- **Archivos de Respaldo**: Se crean automÃ¡ticamente antes de modificaciones
- **Logging Detallado**: Cada error incluye contexto completo
- **Validaciones Cruzadas**: VerificaciÃ³n en cada paso
- **Rollback AutomÃ¡tico**: En caso de fallas crÃ­ticas

---

## ğŸ¯ Resultados Esperados

### ğŸ“Š **Archivos Finales**

1. **`Final_Puntos_{periodo}.csv`**: Todas las propiedades vÃ¡lidas con ubicaciÃ³n precisa
2. **`tablero_maestro_colonias.csv`**: Cobertura completa de 1,062 colonias
3. **Reportes EstadÃ­sticos**: AnÃ¡lisis detallados por variable y colonia

### ğŸ† **GarantÃ­as de Calidad**

- âœ… **100% de precios vÃ¡lidos** (con conversiÃ³n USD)
- âœ… **99.9% de coordenadas vÃ¡lidas**
- âœ… **96.1% de propiedades con colonia asignada**
- âœ… **Consistencia geoespacial completa**
- âœ… **EliminaciÃ³n total de duplicados**

---

## ğŸ”§ Convenciones TÃ©cnicas

### ğŸ“‹ **Formato de Identificadores**
- **Periodo**: `MesAÃ±o` en formato `Sep25` 
- **ID**: `Periodo_<sha1:8>` basado en combinaciÃ³n (Periodo, Ciudad, operacion, tipo_propiedad, titulo, precio)

### ğŸ—‚ï¸ **Manejo de Eliminaciones**
- **Paso 5**: Propiedades invÃ¡lidas â†’ `Datos_Filtrados/Eliminados/`
- **Paso 6**: Duplicados â†’ `Datos_Filtrados/Duplicados/`
- **Paso 8**: Colonias <5 propiedades â†’ `Datos_Filtrados/Esperando/`

### ğŸ“Š **Ãrbol de DecisiÃ³n EstadÃ­stico**
```
n < 5           -> no_estadistica / Esperando
5 <= n < 10     -> mediana_rango
n >= 10 & |skew| > 1 -> mediana_IQR
n >= 30 & |skew| <= 0.5 -> media_desv
Else             -> mediana_IQR
```

---

## ğŸ”„ Invariantes y Validaciones

### âœ… **GarantÃ­as del Sistema**
- `count(id)` no disminuye entre pasos 1-4
- `id` Ãºnicos despuÃ©s del paso 6
- `PxM2` nulo sÃ³lo si falta `precio` o `area_m2`
- Consistencia entre archivos Final_Num, Final_AME, Final_MKT

### ğŸ§ª **Ejemplo de ValidaciÃ³n**
```python
import pandas as pd, os
per='Sep25'
base='N1_Tratamiento/Consolidados/'+per
num=pd.read_csv(os.path.join(base,f'3a.Consolidado_Num_{per}.csv'))
tex=pd.read_csv(os.path.join(base,f'3b.Consolidado_Tex_{per}.csv'))
assert len(num)==len(tex)  # Sin pÃ©rdidas antes del paso 5
```

---

## ğŸš€ Extensiones Futuras

1. **Integrar `scipy`** para pruebas t / ANOVA en colonias n>=30
2. **Validaciones paramÃ©tricas** de rango (area_m2 < 10000, precio > 1000)
3. **Pipeline orquestado** con un Ãºnico comando
4. **Export GEOJSON** union con estadÃ­sticos para visualizaciÃ³n
5. **NLP avanzado** (detecciÃ³n amenities, marketing score, sentimiento)

---

*Este flujo ha sido optimizado y validado para maximizar la calidad y confiabilidad de los datos inmobiliarios procesados.*

---

**ğŸ”„ ESDATA_Epsilon Pipeline** - *Septiembre 2025* âœ¨
