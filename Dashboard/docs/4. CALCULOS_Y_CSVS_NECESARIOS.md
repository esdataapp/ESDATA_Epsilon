# üìä C√ÅLCULOS Y CSVs NECESARIOS - Dashboard ZMG
## Implementaci√≥n Local con CSVs (Sin Base de Datos)

---

## üéØ **ESTRATEGIA DE IMPLEMENTACI√ìN LOCAL**

### **Ventajas del Enfoque CSV**
- ‚úÖ **Setup r√°pido**: Sin configuraci√≥n de base de datos
- ‚úÖ **Portabilidad**: Funciona en cualquier PC
- ‚úÖ **Integraci√≥n directa**: Usa outputs del pipeline ESDATA_Epsilon
- ‚úÖ **Desarrollo √°gil**: Cambios r√°pidos en c√°lculos
- ‚úÖ **Testing f√°cil**: Datos en archivos legibles

### **Arquitectura Simplificada**
```
ESDATA_Epsilon Pipeline ‚Üí CSVs ‚Üí Backend (Node.js) ‚Üí Frontend (React)
                                     ‚Üì
                              Cache en memoria/archivos
```

---

## üìã **LISTADO COMPLETO DE C√ÅLCULOS NECESARIOS**

### **1. ESTAD√çSTICAS B√ÅSICAS (Para Vista Inicio)**

#### A. KPIs Principales
```python
# Archivo: kpis_principales.csv
calculos_kpis = {
    'total_propiedades': len(df),
    'precio_promedio': df['precio'].mean(),
    'precio_mediana': df['precio'].median(),
    'pxm2_promedio': df['precio_por_m2'].mean(),
    'pxm2_mediana': df['precio_por_m2'].median(),
    'superficie_promedio': df['superficie_m2'].mean(),
    'propiedades_nuevas_30d': len(df[df['fecha_scrape'] >= fecha_30d_atras]),
    'cambio_precio_30d': calcular_cambio_precio_30d(),
    'cambio_pxm2_30d': calcular_cambio_pxm2_30d()
}
```

#### B. Top Colonias por Precio/m¬≤
```python
# Archivo: top_colonias.csv
top_colonias = df.groupby(['colonia', 'municipio']).agg({
    'precio': ['count', 'mean', 'median'],
    'precio_por_m2': ['mean', 'median'],
    'superficie_m2': 'mean'
}).round(2)
```

#### C. Distribuci√≥n por Tipo de Propiedad
```python
# Archivo: distribucion_tipos.csv
distribucion_tipos = df.groupby('tipo_propiedad').agg({
    'precio': ['count', 'mean', 'median'],
    'precio_por_m2': ['mean', 'median'],
    'superficie_m2': 'mean'
}).round(2)
```

### **2. HISTOGRAMAS PRE-CALCULADOS (Para Filtros Din√°micos)**

#### A. Histograma de Precios
```python
# Archivo: histograma_precios.csv
def calcular_histograma_precios(df, segmento='all'):
    # Usar Freedman-Diaconis para n√∫mero √≥ptimo de bins
    q75, q25 = np.percentile(df['precio'], [75, 25])
    iqr = q75 - q25
    bin_width = 2 * iqr / (len(df) ** (1/3))
    n_bins = int((df['precio'].max() - df['precio'].min()) / bin_width)
    n_bins = min(max(n_bins, 10), 50)  # Entre 10 y 50 bins
    
    bins, edges = np.histogram(df['precio'], bins=n_bins)
    
    return pd.DataFrame({
        'bin_min': edges[:-1],
        'bin_max': edges[1:],
        'count': bins,
        'percentage': (bins / len(df)) * 100,
        'label': [f'${int(edges[i]/1000)}K - ${int(edges[i+1]/1000)}K' 
                 for i in range(len(edges)-1)]
    })
```

#### B. Histograma de Superficie
```python
# Archivo: histograma_superficie.csv
# Similar al de precios pero para superficie_m2
```

#### C. Histograma de Precio por m¬≤
```python
# Archivo: histograma_pxm2.csv
# Similar al de precios pero para precio_por_m2
```

### **3. ESTAD√çSTICAS POR SEGMENTOS (Para Vista Segmentos)**

#### A. Segmentos Predefinidos
```python
# Archivo: segmentos_predefinidos.csv
segmentos = {
    'starter_1r_1b': {
        'filtros': {'recamaras': [1], 'banos': [1, 1.5]},
        'nombre': 'Starter (1R + 1-1.5B)'
    },
    'family_2r_2b': {
        'filtros': {'recamaras': [2], 'banos': [2, 2.5]},
        'nombre': 'Familiar (2R + 2-2.5B)'
    },
    'premium_3r_3b': {
        'filtros': {'recamaras': [3], 'banos': [3, 3.5]},
        'nombre': 'Premium (3R + 3-3.5B)'
    }
}

def calcular_estadisticas_segmento(df, filtros):
    df_segmento = aplicar_filtros(df, filtros)
    return {
        'count': len(df_segmento),
        'precio_p25': df_segmento['precio'].quantile(0.25),
        'precio_mediana': df_segmento['precio'].median(),
        'precio_p75': df_segmento['precio'].quantile(0.75),
        'superficie_p25': df_segmento['superficie_m2'].quantile(0.25),
        'superficie_mediana': df_segmento['superficie_m2'].median(),
        'superficie_p75': df_segmento['superficie_m2'].quantile(0.75),
        'pxm2_p25': df_segmento['precio_por_m2'].quantile(0.25),
        'pxm2_mediana': df_segmento['precio_por_m2'].median(),
        'pxm2_p75': df_segmento['precio_por_m2'].quantile(0.75)
    }
```

#### B. Distribuci√≥n Geogr√°fica por Segmento
```python
# Archivo: segmentos_por_colonia.csv
def distribucion_geografica_segmento(df, filtros):
    df_segmento = aplicar_filtros(df, filtros)
    return df_segmento.groupby(['colonia', 'municipio']).agg({
        'precio': ['count', 'median'],
        'precio_por_m2': 'median'
    }).round(2)
```

### **4. CORRELACIONES (Para Vista Anal√≠tica)**

#### A. Matriz de Correlaci√≥n
```python
# Archivo: matriz_correlaciones.csv
variables_numericas = [
    'precio', 'superficie_m2', 'precio_por_m2', 
    'recamaras', 'banos', 'estacionamientos', 
    'antiguedad_anos'
]

def calcular_correlaciones(df, segmento='all'):
    df_clean = df[variables_numericas].dropna()
    
    # Pearson correlation
    corr_pearson = df_clean.corr(method='pearson')
    
    # Spearman correlation (para variables no normales)
    corr_spearman = df_clean.corr(method='spearman')
    
    # Convertir a formato largo para el dashboard
    correlaciones = []
    for i, var1 in enumerate(variables_numericas):
        for j, var2 in enumerate(variables_numericas):
            if i < j:  # Solo tri√°ngulo superior
                correlaciones.append({
                    'variable_1': var1,
                    'variable_2': var2,
                    'pearson_r': corr_pearson.loc[var1, var2],
                    'spearman_rho': corr_spearman.loc[var1, var2],
                    'abs_correlation': abs(corr_pearson.loc[var1, var2]),
                    'interpretation': interpretar_correlacion(corr_pearson.loc[var1, var2])
                })
    
    return pd.DataFrame(correlaciones).sort_values('abs_correlation', ascending=False)
```

### **5. AN√ÅLISIS DE AMENIDADES (Para Vista Anal√≠tica)**

#### A. Impacto de Amenidades en Precio
```python
# Archivo: amenidades_impacto.csv
def analizar_amenidades(df):
    amenidades_comunes = [
        'piscina', 'gym', 'terraza', 'jardin', 'roof_garden',
        'seguridad_24h', 'elevador', 'estacionamiento_visitas',
        'salon_eventos', 'area_juegos', 'pet_friendly'
    ]
    
    resultados = []
    for amenidad in amenidades_comunes:
        # Propiedades con y sin amenidad
        con_amenidad = df[df['amenidades'].str.contains(amenidad, na=False)]
        sin_amenidad = df[~df['amenidades'].str.contains(amenidad, na=False)]
        
        if len(con_amenidad) >= 10 and len(sin_amenidad) >= 10:
            # Calcular lift en precio
            precio_con = con_amenidad['precio'].median()
            precio_sin = sin_amenidad['precio'].median()
            lift_absoluto = precio_con - precio_sin
            lift_porcentaje = (lift_absoluto / precio_sin) * 100
            
            # Test estad√≠stico (Mann-Whitney U)
            from scipy.stats import mannwhitneyu
            statistic, p_value = mannwhitneyu(
                con_amenidad['precio'], 
                sin_amenidad['precio'], 
                alternative='two-sided'
            )
            
            resultados.append({
                'amenidad': amenidad,
                'count_con_amenidad': len(con_amenidad),
                'count_sin_amenidad': len(sin_amenidad),
                'prevalencia_rate': len(con_amenidad) / len(df),
                'precio_mediano_con': precio_con,
                'precio_mediano_sin': precio_sin,
                'lift_absoluto': lift_absoluto,
                'lift_porcentaje': lift_porcentaje,
                'p_value': p_value,
                'significativo': p_value < 0.05
            })
    
    return pd.DataFrame(resultados).sort_values('lift_porcentaje', ascending=False)
```

### **6. CLUSTERING DE COLONIAS (Para Vista Anal√≠tica)**

#### A. Clusters por Caracter√≠sticas
```python
# Archivo: clusters_colonias.csv
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

def clustering_colonias(df):
    # Agregar por colonia
    colonias_stats = df.groupby(['colonia', 'municipio']).agg({
        'precio': 'median',
        'precio_por_m2': 'median',
        'superficie_m2': 'median',
        'recamaras': 'mode',
        'banos': 'median'
    }).reset_index()
    
    # Preparar features para clustering
    features = ['precio', 'precio_por_m2', 'superficie_m2', 'recamaras', 'banos']
    X = colonias_stats[features].fillna(colonias_stats[features].median())
    
    # Estandarizar
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # K-Means clustering
    kmeans = KMeans(n_clusters=5, random_state=42)
    clusters = kmeans.fit_predict(X_scaled)
    
    colonias_stats['cluster'] = clusters
    colonias_stats['cluster_nombre'] = colonias_stats['cluster'].map({
        0: 'Econ√≥mico Compacto',
        1: 'Medio Est√°ndar', 
        2: 'Premium Amplio',
        3: 'Luxury Exclusivo',
        4: 'Vertical Moderno'
    })
    
    return colonias_stats
```

### **7. DATOS GEOESPACIALES (Para Vista Explorar)**

#### A. Datos para Mapa de Calor
```python
# Archivo: mapa_calor_colonias.csv
def datos_mapa_calor(df):
    # Agregar por colonia con coordenadas
    mapa_data = df.groupby(['colonia', 'municipio']).agg({
        'precio': ['count', 'median'],
        'precio_por_m2': 'median',
        'latitud': 'mean',  # Centroide de la colonia
        'longitud': 'mean',
        'superficie_m2': 'median'
    }).reset_index()
    
    # Calcular percentiles para colores
    mapa_data['pxm2_percentile'] = mapa_data[('precio_por_m2', 'median')].rank(pct=True)
    mapa_data['color_intensity'] = (mapa_data['pxm2_percentile'] * 100).round(0)
    
    return mapa_data
```

#### B. Clustering para Mapa (Zoom Levels)
```python
# Archivo: mapa_clusters_zoom.csv
def generar_clusters_mapa(df):
    # Para diferentes niveles de zoom
    zoom_configs = {
        'zoom_low': {'radius_km': 5, 'min_points': 50},
        'zoom_medium': {'radius_km': 2, 'min_points': 20},
        'zoom_high': {'radius_km': 0.5, 'min_points': 5}
    }
    
    clusters_por_zoom = {}
    for zoom, config in zoom_configs.items():
        # Implementar clustering geogr√°fico
        clusters = clustering_geografico(df, config['radius_km'], config['min_points'])
        clusters_por_zoom[zoom] = clusters
    
    return clusters_por_zoom
```

### **8. SERIES TEMPORALES (Para Tendencias)**

#### A. Evoluci√≥n Mensual de Precios
```python
# Archivo: series_temporales_precios.csv
def calcular_series_temporales(df):
    # Agrupar por mes
    df['year_month'] = df['fecha_scrape'].dt.to_period('M')
    
    series_zmg = df.groupby('year_month').agg({
        'precio': ['count', 'median'],
        'precio_por_m2': 'median',
        'superficie_m2': 'median'
    }).reset_index()
    
    # Calcular cambios porcentuales
    series_zmg['cambio_precio_pct'] = series_zmg[('precio', 'median')].pct_change() * 100
    series_zmg['cambio_pxm2_pct'] = series_zmg[('precio_por_m2', 'median')].pct_change() * 100
    
    return series_zmg
```

#### B. Series por Municipio
```python
# Archivo: series_por_municipio.csv
# Similar pero agrupando tambi√©n por municipio
```

---

## üìÅ **ESTRUCTURA DE ARCHIVOS CSV NECESARIOS**

### **Directorio: `/Dashboard/data/`**

```
data/
‚îú‚îÄ‚îÄ üìä B√ÅSICOS/
‚îÇ   ‚îú‚îÄ‚îÄ kpis_principales.csv
‚îÇ   ‚îú‚îÄ‚îÄ top_colonias.csv
‚îÇ   ‚îú‚îÄ‚îÄ distribucion_tipos.csv
‚îÇ   ‚îî‚îÄ‚îÄ quick_stats.csv
‚îÇ
‚îú‚îÄ‚îÄ üìà HISTOGRAMAS/
‚îÇ   ‚îú‚îÄ‚îÄ histograma_precios_all.csv
‚îÇ   ‚îú‚îÄ‚îÄ histograma_superficie_all.csv
‚îÇ   ‚îú‚îÄ‚îÄ histograma_pxm2_all.csv
‚îÇ   ‚îú‚îÄ‚îÄ histograma_precios_por_tipo.csv
‚îÇ   ‚îî‚îÄ‚îÄ histograma_precios_por_municipio.csv
‚îÇ
‚îú‚îÄ‚îÄ üéØ SEGMENTOS/
‚îÇ   ‚îú‚îÄ‚îÄ segmentos_predefinidos.csv
‚îÇ   ‚îú‚îÄ‚îÄ segmentos_por_colonia.csv
‚îÇ   ‚îú‚îÄ‚îÄ comparacion_segmentos.csv
‚îÇ   ‚îî‚îÄ‚îÄ rangos_recomendados.csv
‚îÇ
‚îú‚îÄ‚îÄ üîó CORRELACIONES/
‚îÇ   ‚îú‚îÄ‚îÄ matriz_correlaciones_all.csv
‚îÇ   ‚îú‚îÄ‚îÄ correlaciones_por_tipo.csv
‚îÇ   ‚îî‚îÄ‚îÄ correlaciones_significativas.csv
‚îÇ
‚îú‚îÄ‚îÄ üè† AMENIDADES/
‚îÇ   ‚îú‚îÄ‚îÄ amenidades_impacto.csv
‚îÇ   ‚îú‚îÄ‚îÄ amenidades_por_tipo.csv
‚îÇ   ‚îî‚îÄ‚îÄ amenidades_por_municipio.csv
‚îÇ
‚îú‚îÄ‚îÄ üó∫Ô∏è GEOESPACIAL/
‚îÇ   ‚îú‚îÄ‚îÄ mapa_calor_colonias.csv
‚îÇ   ‚îú‚îÄ‚îÄ clusters_geograficos.csv
‚îÇ   ‚îú‚îÄ‚îÄ centroides_colonias.csv
‚îÇ   ‚îî‚îÄ‚îÄ boundaries_simplificados.geojson
‚îÇ
‚îú‚îÄ‚îÄ üìÖ SERIES_TEMPORALES/
‚îÇ   ‚îú‚îÄ‚îÄ series_zmg_mensual.csv
‚îÇ   ‚îú‚îÄ‚îÄ series_por_municipio.csv
‚îÇ   ‚îî‚îÄ‚îÄ series_por_tipo.csv
‚îÇ
‚îî‚îÄ‚îÄ üîç FILTROS/
    ‚îú‚îÄ‚îÄ opciones_tipos_propiedad.csv
    ‚îú‚îÄ‚îÄ opciones_operaciones.csv
    ‚îú‚îÄ‚îÄ opciones_municipios.csv
    ‚îú‚îÄ‚îÄ opciones_colonias.csv
    ‚îî‚îÄ‚îÄ rangos_sugeridos.csv
```

---

## üêç **SCRIPT PYTHON PARA GENERAR TODOS LOS CSVs**

```python
# generate_dashboard_data.py
import pandas as pd
import numpy as np
from pathlib import Path
import json

class DashboardDataGenerator:
    def __init__(self, input_csv_path, output_dir):
        self.df = pd.read_csv(input_csv_path)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # Crear subdirectorios
        for subdir in ['basicos', 'histogramas', 'segmentos', 'correlaciones', 
                      'amenidades', 'geoespacial', 'series_temporales', 'filtros']:
            (self.output_dir / subdir).mkdir(exist_ok=True)
    
    def generar_todos_los_csvs(self):
        print("üöÄ Generando todos los CSVs para el dashboard...")
        
        # 1. B√°sicos
        self.generar_kpis_principales()
        self.generar_top_colonias()
        self.generar_distribucion_tipos()
        
        # 2. Histogramas
        self.generar_histogramas()
        
        # 3. Segmentos
        self.generar_segmentos()
        
        # 4. Correlaciones
        self.generar_correlaciones()
        
        # 5. Amenidades
        self.generar_amenidades()
        
        # 6. Geoespacial
        self.generar_datos_mapa()
        
        # 7. Series temporales
        self.generar_series_temporales()
        
        # 8. Filtros
        self.generar_opciones_filtros()
        
        print("‚úÖ Todos los CSVs generados exitosamente!")
    
    # Implementar cada m√©todo...
```

---

## üöÄ **PR√ìXIMOS PASOS PARA IMPLEMENTACI√ìN**

### **1. Crear el Script Generador (Hoy)**
- Implementar `generate_dashboard_data.py`
- Integrar con el pipeline ESDATA_Epsilon existente
- Generar todos los CSVs iniciales

### **2. Backend Simplificado (Ma√±ana)**
- Eliminar Prisma/PostgreSQL
- Crear servicio que lea CSVs
- Endpoints que devuelvan JSON desde CSVs

### **3. Frontend MVP (Esta semana)**
- 2 vistas: Inicio + Explorar
- Consumir endpoints JSON
- Filtros b√°sicos funcionando

¬øTe parece bien este enfoque? ¬øQuieres que empiece implementando el script generador de CSVs?
