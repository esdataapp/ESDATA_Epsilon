###################################################################################################
# FLUJO DASHBOARD STREAMLIT                                                                       #
# Proyecto: ESDATA_Epsilon                                                                         #
# Objetivo: Documentar la cadena de generación de artefactos y su consumo en la UI                #
###################################################################################################

ÍNDICE
 1. Propósito del Dashboard
 2. Artefactos de Entrada (CSV derivados)
 3. Proceso de Generación (generate_dashboard_data.py)
 4. Estructura Lógica de la App (app.py)
 5. Métricas y Secciones Clave
 6. Caching, Performance y Sampling
 7. Descargas y Exportaciones
 8. Buenas Prácticas de Diseño UX/Analítica
 9. Extensiones Futuras

===================================================================================================
1. PROPÓSITO
===================================================================================================
Visor analítico exploratorio para:
- Evaluar métricas por colonia (precio, PxM2, área).
- Medir diferenciación de amenidades (lift vs ratio global).
- Analizar texto y señales de marketing (frecuencias, TF-IDF, nube de palabras).
- Explorar distribución, outliers y densidad (KDE).
- Evaluar confianza de colonias (score compuesto) y priorizar comparables.
- Profundizar en variable "area_m2" (correlaciones, estratos, amenidades, histogramas).

===================================================================================================
2. ARTEFACTOS DE ENTRADA
===================================================================================================
Generados en Dashboard/CSV/<Periodo>/ por generate_dashboard_data.py:
- colony_stats.csv
- colony_quantiles.csv
- outliers_flagged.csv
- price_area_heatmap_long.csv / price_area_heatmap_matrix.csv
- colony_distribution_long.csv
- amenity_prevalence.csv (o fallback autogenerado)
- marketing_signals.csv (si existe 0.Final_MKT)
- pxm2_evolution_stub.csv (placeholder temporal)
Requiere además bases finales:
- N5_Resultados/Nivel_1/CSV/0.Final_Num_<Periodo>.csv
- N5_Resultados/Nivel_1/CSV/0.Final_Ame_<Periodo>.csv
- (Opcional) 0.Final_MKT_<Periodo>.csv

===================================================================================================
3. GENERACIÓN (generate_dashboard_data.py)
===================================================================================================
Pipeline principal:
  a) Carga 0.Final_Num y crea estadísticos colonia (groupby Ciudad-Colonia-operacion-tipo_propiedad)
  b) Calcula cuantiles p10/p25/p50/p75/p90
  c) Flag outliers (IQR) y exporta outliers_flagged.csv
  d) Construye heatmap área-precio (long + matriz pivot para densidad)
  e) Distribución larga (colonia_distribution_long.csv) para histogramas estratificados
  f) Amenity prevalence (si no existe, fallback a partir de 0.Final_Ame) -> amenity_prevalence.csv
  g) Marketing signals (si 0.Final_MKT existe) -> marketing_signals.csv
  h) Stub evolución PxM2 (placeholder estructural)
  i) Persistencia en Dashboard/CSV/<Periodo>/
Control de errores: try/except por bloque con logging simple.

===================================================================================================
4. ESTRUCTURA DE LA APP (app.py)
===================================================================================================
Secciones principales (orden):
  1. Parámetros / Filtros (periodo, ciudad, operación, tipo)
  2. KPIs & Score Confianza (Badges por colonia) – compute_colony_confidence
  3. Distribuciones (histogramas, KDE toggle, PxM2/Precio/Area)
  4. Outliers (tabla + límites IQR)
  5. Diferenciación Amenidades (amenity_differentiation: heat + ranking)
  6. Texto & Marketing (word frequencies, TF-IDF terciles PxM2, wordcloud)
  7. Geoespacial (pydeck map + métricas hover)
  8. Área Exhaustiva (area_correlations, area_stratification, colony_area_correlations, amenity_area_effect)
  9. Descargas (datasets clave filtrados)
 10. Diagnostics (tamaños de datasets, sampling aplicado)

===================================================================================================
5. MÉTRICAS Y SECCIONES CLAVE
===================================================================================================
Score Confianza (0–100): volumen + pct_outliers + iqr_ratio + cv.
Amenity Lift: ratio_colonia / ratio_global con top-N filtrado.
KDE Overlay: Opción en histogramas para suavizar densidad.
Stratificación Área: Bins fijos (ver backend) y conteos / mediana PxM2.
Correlaciones por Colonia: filtro min_n y top ±30.
Amenity Area Effect: diff mediana area (present - absent) top descendente.

===================================================================================================
6. CACHING / PERFORMANCE
===================================================================================================
- Sampling: si final_num_filtered > 50k, limitar a 50k para módulo área.
- Posible futura mejora: @st.cache_data en correlaciones por colonia.
- Heavy ops (TF-IDF, wordcloud) ejecutadas solo al expandir sección.

===================================================================================================
7. DESCARGAS / EXPORTACIONES
===================================================================================================
Botones actuales:
- Correlaciones por colonia (CSV)
- Efecto amenidades área (CSV)
Posibles: stratificación completa, correlaciones globales, amenity ranking.

===================================================================================================
8. UX / ANÁLISIS – BUENAS PRÁCTICAS
===================================================================================================
- Mostrar siempre conteo de registros usados (post-filtro) para contexto.
- Etiquetas claras y consistentes (PxM2, Área m², Precio MN).
- Usar badges de color para niveles de confianza.
- Filtrar amenidades a top-N para heatmaps legibles.
- Expanders para secciones pesadas (Texto, Área) → carga diferida.

===================================================================================================
9. EXTENSIONES FUTURAS
===================================================================================================
- Evolución temporal real (series históricas PxM2 por colonia).
- Comparador de colonias (side-by-side KPIs, radar chart).
- Segmentación automática (clustering colonias multi-métricas).
- Integración embeddings (búsqueda semántica de listings similares).
- Reporte PDF o Markdown autogenerado (plantillas Jinja2).
- Análisis de co-ocurrencia de amenidades (network graph).

===================================================================================================
10. PSEUDOCÓDIGO DETALLADO generate_dashboard_data
===================================================================================================
Input: raiz_periodo = "Dashboard/CSV/<Periodo>" (crear si no existe)

Pseudocódigo resumido:
load_final_num(path):
  df = read_csv(path, dtype=esquema_min)
  limpiar nulos críticos (precio, area_m2)
  df['pxm2'] = df['precio']/df['area_m2']
  derivar estrato_area si no existe
  return df

main(periodo):
  crear carpeta raiz_periodo
  df_num = load_final_num(0.Final_Num_<Periodo>)
  df_amen = opcional_load(0.Final_Ame_<Periodo>)
  df_mkt  = opcional_load(0.Final_MKT_<Periodo>)
  colony_stats = groupby_stats(df_num)
  quantiles = compute_quantiles(df_num, ['pxm2','precio','area_m2'])
  outliers = detect_outliers(df_num)
  heat_long, heat_matrix = build_heatmap(df_num)
  distrib_long = build_distribution(df_num)
  amen_prev = compute_amenity_prevalence(df_num, df_amen) or infer_amenities(df_num)
  marketing = compute_marketing_signals(df_mkt) or marketing_stub()
  evolution = evolution_stub(df_num)
  save_all(...)
  return lista_paths

Logs sugeridos por paso: step, n_in, n_out, duracion_ms, warnings.

===================================================================================================
11. DEPENDENCIAS Y ORDEN DE CARGA
===================================================================================================
Orden óptimo en app:
 1. Carga base final_num (obligatoria)
 2. Derivar pxm2 & estratos (si faltan)
 3. Cargar colony_stats + quantiles -> panel KPIs
 4. Cargar heatmaps y distribution on-demand (expanders)
 5. Cargar amenity_prevalence y marketing_signals lazy
 6. Outliers sólo si sección activa

===================================================================================================
12. ABORT CONDITIONS / FAILSAFE
===================================================================================================
Abortar render principal si:
 - Falta 0.Final_Num_<Periodo>.csv o está vacío
 - >20% nulos en precio o area_m2 tras limpieza
 - pxm2 genera >5% valores no finitos
Advertir (no abortar) si:
 - Colonias con n<3 (marcar baja confiabilidad)
 - Amenity prevalence faltante (usar fallback)

Fallbacks definidos:
 - Amenidades: inferir de columnas amen_*
 - Marketing: stub vacía
 - Evolución: stub estructura mínima

===================================================================================================
13. RUTAS DE ARCHIVO Y NAMING
===================================================================================================
Entradas núcleo:
 - N5_Resultados/Nivel_1/CSV/0.Final_Num_<Periodo>.csv
 - N5_Resultados/Nivel_1/CSV/0.Final_Ame_<Periodo>.csv
 - N5_Resultados/Nivel_1/CSV/0.Final_MKT_<Periodo>.csv (opcional)

Salidas dashboard:
 - Dashboard/CSV/<Periodo>/colony_stats.csv
 - Dashboard/CSV/<Periodo>/colony_quantiles.csv
 - Dashboard/CSV/<Periodo>/outliers_flagged.csv
 - Dashboard/CSV/<Periodo>/price_area_heatmap_long.csv
 - Dashboard/CSV/<Periodo>/price_area_heatmap_matrix.csv
 - Dashboard/CSV/<Periodo>/colony_distribution_long.csv
 - Dashboard/CSV/<Periodo>/amenity_prevalence.csv
 - Dashboard/CSV/<Periodo>/marketing_signals.csv
 - Dashboard/CSV/<Periodo>/pxm2_evolution_stub.csv

Convenciones:
 - snake_case
 - *_long vs *_matrix para representaciones pivot
 - *_stub para placeholders

===================================================================================================
14. CHECKLIST MANTENIMIENTO / DEPLOY
===================================================================================================
[ ] Validar duplicados id/hash
[ ] Verificar tamaño archivos y optimizar dtypes
[ ] Regenerar artefactos periodo
[ ] Revisar outliers top extremos
[ ] Confirmar amenity_prevalence actualizada
[ ] Actualizar schema JSON si nuevas columnas
[ ] Registrar cambios en changelog interno
[ ] Probar secciones pesadas (Texto, Área) bajo sampling

FIN DEL DOCUMENTO.
